{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e67e713",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wilbertT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import fitz\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
    "from scipy.stats import loguniform\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f7f8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the raw folders (e.g. archive(1)/data/data/ACCOUNTANT/...)\n",
    "PATH_RAW_FOLDERS = \"archive(1)/data/data\"\n",
    "OUTPUT_METRICS_FILE = \"ranking_evaluation_results.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86369c02",
   "metadata": {},
   "source": [
    "##### 2. DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a2488f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy model...\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy\n",
    "print(\"Loading spaCy model...\")\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    # print(\"Warning: spaCy model not found. Using regex cleaning only.\")\n",
    "    nlp = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f28dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleans raw resume text: removes newlines, special chars, extra spaces.\n",
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # Remove newlines and tabs\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # Remove non-ascii characters (clean up messy PDF artifacts)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0882ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders(base_path):\n",
    "    if not os.path.exists(base_path):\n",
    "        raise FileNotFoundError(f\"CRITICAL ERROR: Folder '{base_path}' not found.\")\n",
    "        \n",
    "    print(f\"--- Scanning folders in {base_path} ---\")\n",
    "    data = []\n",
    "    \n",
    "    # Walk through all subfolders\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        category = os.path.basename(root)\n",
    "        \n",
    "        # Skip the base folder itself\n",
    "        if root == base_path: \n",
    "            continue\n",
    "            \n",
    "        pdf_files = [f for f in files if f.lower().endswith(\".pdf\")]\n",
    "        \n",
    "        if pdf_files:\n",
    "            print(f\"Processing {category:<20} | Found {len(pdf_files)} resumes\")\n",
    "            \n",
    "            for file in pdf_files:\n",
    "                file_path = os.path.join(root, file)\n",
    "                try:\n",
    "                    # EXTRACT TEXT WITH FITZ\n",
    "                    text = \"\"\n",
    "                    with fitz.open(file_path) as doc:\n",
    "                        for page in doc:\n",
    "                            text += page.get_text()\n",
    "                    \n",
    "                    # Store clean data\n",
    "                    data.append({\n",
    "                        \"Category\": category, \n",
    "                        \"Filename\": file,\n",
    "                        \"Cleaned_Text\": clean_text(text)\n",
    "                    })\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"  [Error] Could not read {file}: {e}\")\n",
    "    \n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7363992f",
   "metadata": {},
   "source": [
    "##### 3. EVALUATION LOGIC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47e0df5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_ranking_performance(y_true, y_pred_proba, classes, output_path):\n",
    "    n_samples = len(y_true)\n",
    "    ranks = []\n",
    "    reciprocal_ranks = []\n",
    "    detailed_results = []\n",
    "    \n",
    "    print(f\"\\nEvaluating predictions for {n_samples} test resumes...\")\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        true_label = y_true[i]\n",
    "        probs = y_pred_proba[i]\n",
    "        \n",
    "        # Sort predictions by probability (Highest first)\n",
    "        sorted_indices = np.argsort(probs)[::-1]\n",
    "        ranked_classes = [classes[idx] for idx in sorted_indices]\n",
    "        \n",
    "        # Find where the TRUE category is in the ranked list\n",
    "        if true_label in ranked_classes:\n",
    "            rank = ranked_classes.index(true_label) + 1\n",
    "        else:\n",
    "            # Fallback if label is missing (rare)\n",
    "            rank = len(classes) + 1 \n",
    "\n",
    "        reciprocal_rank = 1.0 / rank\n",
    "        ranks.append(rank)\n",
    "        reciprocal_ranks.append(reciprocal_rank)\n",
    "        \n",
    "        detailed_results.append({\n",
    "            \"resume_idx\": i,\n",
    "            \"true_category\": true_label,\n",
    "            \"top1_prediction\": ranked_classes[0],\n",
    "            \"rank_of_true_category\": rank,\n",
    "            \"reciprocal_rank\": reciprocal_rank\n",
    "        })\n",
    "\n",
    "    # --- Calculate Aggregate Metrics ---\n",
    "    mrr = np.mean(reciprocal_ranks)\n",
    "    acc_top1 = np.mean([r == 1 for r in ranks])\n",
    "    acc_top3 = np.mean([r <= 3 for r in ranks])\n",
    "    avg_rank = np.mean(ranks)\n",
    "    \n",
    "    # NDCG@3 Approximation\n",
    "    ndcg_scores = [(1.0 / np.log2(r + 1)) if r <= 3 else 0.0 for r in ranks]\n",
    "    mean_ndcg_3 = np.mean(ndcg_scores)\n",
    "    \n",
    "    # --- PRINT THE REPORT ---\n",
    "    print(\"\\n\" + \"=\"*70 + \"FINAL EVALUATION SUMMARY (All Data, Proper Ranking Metrics)\" + \"=\"*70)\n",
    "    print(f\"Total resumes evaluated: {n_samples}\")\n",
    "    print(f\"Total categories: {len(classes)}\")\n",
    "    print(f\"Mean Reciprocal Rank (MRR):        {mrr:.4f}\")\n",
    "    print(f\"Precision@1 (Accuracy):           {acc_top1:.4f}\")\n",
    "    print(f\"Precision@3:                      {acc_top3:.4f}\")\n",
    "    print(f\"Mean NDCG@3:                      {mean_ndcg_3:.4f}\")\n",
    "    print(f\"Average rank of true category:    {avg_rank:.2f} / {len(classes)}\")\n",
    "    print(\"=\"*180)\n",
    "    \n",
    "    # Save CSV\n",
    "    df_res = pd.DataFrame(detailed_results)\n",
    "    df_res.to_csv(output_path, index=False)\n",
    "    print(f\"Detailed results saved to: {os.path.abspath(output_path)}\")\n",
    "    \n",
    "    # Distribution\n",
    "    correct_count = sum([r == 1 for r in ranks])\n",
    "    print(f\"Correct predictions (P@1): {correct_count} / {n_samples} ({acc_top1*100:.1f}%)\")\n",
    "    \n",
    "    print(\"Distribution of true category ranks:\")\n",
    "    rank_counts = pd.Series(ranks).value_counts().sort_index()\n",
    "    for r in range(1, 11):\n",
    "        c = rank_counts.get(r, 0)\n",
    "        print(f\"  Rank {r:>2}: {c:>4} resumes ({(c/n_samples)*100:>5.1f}%)\")\n",
    "\n",
    "    print(\"\\nWorst predictions (lowest reciprocal rank):\")\n",
    "    print(df_res.sort_values(\"reciprocal_rank\").head(5).to_string(index=False))\n",
    "    \n",
    "    print(\"\\nBest predictions (highest reciprocal rank):\")\n",
    "    print(df_res.sort_values([\"reciprocal_rank\", \"resume_idx\"], ascending=[False, True]).head(5).to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb2d0ed",
   "metadata": {},
   "source": [
    "##### 4. MAIN FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd78b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model (all-mpnet-base-v2)...\n"
     ]
    }
   ],
   "source": [
    "# SBERT setup: install with `pip install -U sentence-transformers` if needed\n",
    "print(\"Loading SBERT model (all-MiniLM-L6-v2)...\")\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "def embed_texts(texts, batch_size=64):\n",
    "    \"\"\"Encode a list of texts into numpy embeddings using SBERT.\"\"\"\n",
    "    return model_sbert.encode(texts, batch_size=batch_size, show_progress_bar=True, convert_to_numpy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a3c3369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scanning folders in archive(1)/data/data ---\n",
      "Processing ACCOUNTANT           | Found 118 resumes\n",
      "Processing ADVOCATE             | Found 118 resumes\n",
      "Processing AGRICULTURE          | Found 63 resumes\n",
      "Processing APPAREL              | Found 97 resumes\n",
      "Processing ARTS                 | Found 103 resumes\n",
      "Processing AUTOMOBILE           | Found 36 resumes\n",
      "Processing AVIATION             | Found 117 resumes\n",
      "Processing BANKING              | Found 115 resumes\n",
      "Processing BPO                  | Found 22 resumes\n",
      "Processing BUSINESS-DEVELOPMENT | Found 120 resumes\n",
      "Processing CHEF                 | Found 118 resumes\n",
      "Processing CONSTRUCTION         | Found 112 resumes\n",
      "Processing CONSULTANT           | Found 115 resumes\n",
      "Processing DESIGNER             | Found 107 resumes\n",
      "Processing DIGITAL-MEDIA        | Found 96 resumes\n",
      "Processing ENGINEERING          | Found 118 resumes\n",
      "Processing FINANCE              | Found 118 resumes\n",
      "Processing FITNESS              | Found 117 resumes\n",
      "Processing HEALTHCARE           | Found 115 resumes\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_18356\\2452801286.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[38;5;66;03m# A. Load Data (Reading PDFs from Folder)\u001b[39;00m\n\u001b[32m      4\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m      5\u001b[39m         df = load_data_from_folders(PATH_RAW_FOLDERS)\n\u001b[32m      6\u001b[39m         print(f\"\\nSuccessfully loaded {len(df)} resumes.\")\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m      8\u001b[39m         print(e)\n\u001b[32m      9\u001b[39m         exit()\n\u001b[32m     10\u001b[39m \n",
      "\u001b[32m~\\AppData\\Local\\Temp\\ipykernel_18356\\4153970758.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(base_path)\u001b[39m\n\u001b[32m     33\u001b[39m                         \u001b[33m\"Filename\"\u001b[39m: file,\n\u001b[32m     34\u001b[39m                         \u001b[33m\"Cleaned_Text\"\u001b[39m: clean_text(text)\n\u001b[32m     35\u001b[39m                     })\n\u001b[32m     36\u001b[39m \n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m                 \u001b[38;5;28;01mexcept\u001b[39;00m Exception \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     38\u001b[39m                     print(f\"  [Error] Could not read {file}: {e}\")\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(data)\n",
      "\u001b[32mc:\\Users\\wilbertT\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pymupdf\\__init__.py\u001b[39m in \u001b[36m?\u001b[39m\u001b[34m(self, filename, stream, filetype, rect, width, height, fontsize)\u001b[39m\n\u001b[32m   3061\u001b[39m                     self.page_count2 = extra.page_count_pdf\n\u001b[32m   3062\u001b[39m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3063\u001b[39m                     self.page_count2 = extra.page_count_fz\n\u001b[32m   3064\u001b[39m         \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3065\u001b[39m             JM_mupdf_show_errors = JM_mupdf_show_errors_old\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # A. Load Data (Reading PDFs from Folder)\n",
    "    try:\n",
    "        df = load_data_from_folders(PATH_RAW_FOLDERS)\n",
    "        print(f\"\\nSuccessfully loaded {len(df)} resumes.\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        exit()\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"Error: No PDF files found. Check your folder path.\")\n",
    "        exit()\n",
    "\n",
    "    # B. Split Data\n",
    "    print(\"Splitting data into Train/Test sets...\")\n",
    "    X = df['Cleaned_Text']\n",
    "    y = df['Category']\n",
    "\n",
    "    # Stratify ensures we have examples of every job type in the test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # C1. TF-IDF Features\n",
    "    print(\"Fitting TF-IDF vectorizer on training data...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=20000, ngram_range=(1, 2))\n",
    "    tfidf_train = tfidf_vectorizer.fit_transform(X_train.values.astype('U'))\n",
    "    tfidf_test = tfidf_vectorizer.transform(X_test.values.astype('U'))\n",
    "\n",
    "    # C2. Embedding (SBERT)\n",
    "    print(\"Embedding texts with SBERT model...\")\n",
    "    # Convert pandas Series to list and embed\n",
    "    X_train_vec = embed_texts(X_train.tolist())\n",
    "    X_test_vec = embed_texts(X_test.tolist())\n",
    "\n",
    "    # Combine SBERT embeddings with TF-IDF features (sparse-safe)\n",
    "    print(\"Combining SBERT embeddings with TF-IDF features using sparse hstack...\")\n",
    "    # Convert dense SBERT embeddings to sparse and horizontally stack with TF-IDF sparse matrices\n",
    "    X_train_vec_sparse = sparse.csr_matrix(X_train_vec)\n",
    "    X_test_vec_sparse = sparse.csr_matrix(X_test_vec)\n",
    "\n",
    "    X_train_combined = sparse.hstack([X_train_vec_sparse, tfidf_train]).tocsr()\n",
    "    X_test_combined = sparse.hstack([X_test_vec_sparse, tfidf_test]).tocsr()\n",
    "\n",
    "    # D. Hyperparameter tuning + Train Model (Logistic Regression)\n",
    "    print(\"Running RandomizedSearchCV to tune LogisticRegression (C parameter)...\")\n",
    "    # Use class_weight='balanced' to mitigate class imbalance\n",
    "    base_clf = LogisticRegression(max_iter=2000, multi_class='multinomial', solver='saga', n_jobs=-1, class_weight='balanced')\n",
    "    param_dist = {'C': loguniform(1e-4, 1e4)}\n",
    "    search = RandomizedSearchCV(\n",
    "    base_clf, \n",
    "    param_distributions=param_dist, \n",
    "    n_iter=4,     # Reduce from 20 to 4 (checks fewer random settings)\n",
    "    cv=3,         # Reduce from 5 to 3 (less validation, fine for demos)\n",
    "    scoring='accuracy', \n",
    "    n_jobs=-1, \n",
    "    random_state=42\n",
    ")\n",
    "    search.fit(X_train_combined, y_train)\n",
    "    clf = search.best_estimator_\n",
    "    print(\"Best params:\", search.best_params_)\n",
    "\n",
    "    # E. Predict Probabilities (Crucial for Ranking)\n",
    "    print(\"Predicting probabilities on test set...\")\n",
    "    y_pred_proba = clf.predict_proba(X_test_combined)\n",
    "    classes = clf.classes_\n",
    "\n",
    "    # F. Run Evaluation\n",
    "    evaluate_ranking_performance(\n",
    "        y_true=y_test.values,\n",
    "        y_pred_proba=y_pred_proba,\n",
    "        classes=classes,\n",
    "        output_path=OUTPUT_METRICS_FILE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ec87751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SBERT model (all-MiniLM-L6-v2)...\n",
      "\n",
      "--- CALCULATING MATCHING SCORE ---\n",
      "Job Description Length: 287 chars\n",
      "Resume Length: 261 chars\n",
      "----------------------------------------\n",
      "Matching Score: 0.7026\n",
      "Match Percentage: 70.26%\n",
      "----------------------------------------\n",
      ">> VERDICT: Good Match\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & LOAD MODEL\n",
    "# ==========================================\n",
    "print(\"Loading SBERT model (all-MiniLM-L6-v2)...\")\n",
    "# This model handles the \"Semantic\" understanding of the text\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# ==========================================\n",
    "# 2. PRE-PROCESSING FUNCTIONS\n",
    "# ==========================================\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Cleans text to ensure fair comparison.\n",
    "    (Corresponds to 'Skill Normalization' concept in your image)\n",
    "    \"\"\"\n",
    "    if not text: return \"\"\n",
    "    \n",
    "    # Remove newlines and tabs\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    # Remove non-ascii characters\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    \n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    \n",
    "    return text.strip().lower()\n",
    "\n",
    "def calculate_matching_score(job_description, resume_text):\n",
    "    \"\"\"\n",
    "    Calculates the semantic similarity score between a Job and a Resume.\n",
    "    \"\"\"\n",
    "    # A. Pre-process (Clean & Normalize)\n",
    "    clean_jd = clean_text(job_description)\n",
    "    clean_resume = clean_text(resume_text)\n",
    "\n",
    "    # B. Embed (Convert text to Semantic Vectors)\n",
    "    # This replaces \"Skill Extraction\" by capturing the entire context/skills semantically\n",
    "    embeddings = model_sbert.encode([clean_jd, clean_resume])\n",
    "    \n",
    "    # embeddings[0] is the Job Description\n",
    "    # embeddings[1] is the Resume\n",
    "\n",
    "    # C. Matching (Cosine Similarity)\n",
    "    # Reshape is needed because cosine_similarity expects 2D arrays\n",
    "    # This calculates how close the two vectors are in space\n",
    "    score = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
    "    \n",
    "    return score\n",
    "\n",
    "# ==========================================\n",
    "# 3. RUN THE MATCHING SYSTEM\n",
    "# ==========================================\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # --- INPUT 1: The Job Description ---\n",
    "    job_desc = \"\"\"\n",
    "    We are looking for a Senior Python Developer.\n",
    "    Requirements:\n",
    "    - Strong experience in Python and Django/Flask.\n",
    "    - Knowledge of SQL and NoSQL databases.\n",
    "    - Experience with REST APIs and Cloud services (AWS).\n",
    "    - Understanding of machine learning concepts is a plus.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- INPUT 2: The Candidate Resume ---\n",
    "    candidate_resume = \"\"\"\n",
    "    Experienced Software Engineer with a focus on backend systems.\n",
    "    Proficient in Python, Java, and C++.\n",
    "    Built scalable web applications using Django and PostgreSQL.\n",
    "    Familiar with AWS (EC2, S3) and Docker.\n",
    "    Interested in Data Science and AI.\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\n--- CALCULATING MATCHING SCORE ---\")\n",
    "    print(f\"Job Description Length: {len(job_desc)} chars\")\n",
    "    print(f\"Resume Length: {len(candidate_resume)} chars\")\n",
    "    \n",
    "    # Run the comparison\n",
    "    similarity_score = calculate_matching_score(job_desc, candidate_resume)\n",
    "    \n",
    "    # Output the result\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Matching Score: {similarity_score:.4f}\")\n",
    "    print(f\"Match Percentage: {similarity_score * 100:.2f}%\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "    # Interpretation\n",
    "    if similarity_score > 0.75:\n",
    "        print(\">> VERDICT: Excellent Match\")\n",
    "    elif similarity_score > 0.5:\n",
    "        print(\">> VERDICT: Good Match\")\n",
    "    else:\n",
    "        print(\">> VERDICT: Low Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e00cc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifacts to model_artifacts\n"
     ]
    }
   ],
   "source": [
    "# After training finishes and you have `clf` and `tfidf_vectorizer`\n",
    "from resume_pipeline import save_artifacts\n",
    "import json, os\n",
    "\n",
    "outdir = \"model_artifacts\"\n",
    "save_artifacts(outdir, clf, tfidf_vectorizer)\n",
    "\n",
    "# optional metadata (class ordering)\n",
    "os.makedirs(outdir, exist_ok=True)\n",
    "with open(os.path.join(outdir, \"metadata.json\"), \"w\", encoding=\"utf8\") as fh:\n",
    "    json.dump({\"classes\": clf.classes_.tolist()}, fh)\n",
    "\n",
    "print(\"Saved artifacts to\", outdir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
