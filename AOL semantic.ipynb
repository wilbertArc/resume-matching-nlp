{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6fe211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# 1. Path to your Resume folders (PDFs)\n",
    "PATH_RESUMES = \"archive(1)/data/data\"\n",
    "\n",
    "# 2. Path to your Job Description folder (CSV files)\n",
    "PATH_JOBS = \"jobdesc/home/sdf\"\n",
    "\n",
    "OUTPUT_FILE = \"processed_data.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c251c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    text = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    return re.sub(' +', ' ', text).strip()\n",
    "\n",
    "def load_resumes(base_path):\n",
    "    print(f\"--- Scanning Resumes in {base_path} ---\")\n",
    "    data = []\n",
    "    total_files = 0\n",
    "    \n",
    "    # Walk through all folders (Accountant, Engineering, etc.)\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        category = os.path.basename(root)\n",
    "        if root == base_path: continue\n",
    "        \n",
    "        pdf_files = [f for f in files if f.lower().endswith(\".pdf\")]\n",
    "        \n",
    "        for file in pdf_files:\n",
    "            try:\n",
    "                # Read PDF\n",
    "                full_path = os.path.join(root, file)\n",
    "                with fitz.open(full_path) as doc:\n",
    "                    text = \"\".join([page.get_text() for page in doc])\n",
    "                \n",
    "                # Only save if it has text\n",
    "                if len(text) > 50:\n",
    "                    data.append({\n",
    "                        \"Filename\": file, \n",
    "                        \"Category\": category, \n",
    "                        \"Text\": clean_text(text),\n",
    "                        \"Path\": full_path\n",
    "                    })\n",
    "                    total_files += 1\n",
    "                    \n",
    "                    # Print progress every 100 files\n",
    "                    if total_files % 100 == 0:\n",
    "                        print(f\"Processed {total_files} resumes...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                \n",
    "    print(f\"Done! Loaded {len(data)} total resumes.\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def load_jobs(base_path):\n",
    "    print(f\"--- Scanning Jobs in {base_path} ---\")\n",
    "    all_jobs = []\n",
    "    if os.path.exists(base_path):\n",
    "        csv_files = [f for f in os.listdir(base_path) if f.lower().endswith('.csv')]\n",
    "        for file in csv_files:\n",
    "            try:\n",
    "                df = pd.read_csv(os.path.join(base_path, file)).fillna(\"\")\n",
    "                \n",
    "                # Auto-detect columns\n",
    "                text_col = next((c for c in df.columns if 'description' in c.lower()), None)\n",
    "                title_col = next((c for c in df.columns if 'title' in c.lower()), None)\n",
    "                \n",
    "                if text_col:\n",
    "                    temp = pd.DataFrame()\n",
    "                    temp['Job_Title'] = df[title_col] if title_col else \"Job \" + df.index.astype(str)\n",
    "                    temp['Job_Text'] = df[text_col].apply(clean_text)\n",
    "                    all_jobs.append(temp)\n",
    "                    print(f\"Loaded jobs from {file}\")\n",
    "            except: pass\n",
    "            \n",
    "    return pd.concat(all_jobs, ignore_index=True) if all_jobs else pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11e74118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_job_descriptions(base_path):\n",
    "    \"\"\"\n",
    "    Scans the folder for CSV files and loads Job Descriptions.\n",
    "    It attempts to find columns like 'Job Description', 'Description', or 'Job Title'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Warning: Job Desc folder '{base_path}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"--- Scanning Job Descriptions in {base_path} ---\")\n",
    "    all_jobs = []\n",
    "\n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(base_path) if f.lower().endswith('.csv')]\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(base_path, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Loaded {file} with columns: {list(df.columns)}\")\n",
    "            \n",
    "            # --- INTELLIGENT COLUMN DETECTION ---\n",
    "            # We need to find which column contains the text we want to match.\n",
    "            # We look for common names.\n",
    "            text_col = None\n",
    "            title_col = None\n",
    "            \n",
    "            # 1. Find the Text Column\n",
    "            candidates_text = ['Job Description', 'Description', 'job_description', 'description', 'Job Text']\n",
    "            for col in candidates_text:\n",
    "                if col in df.columns:\n",
    "                    text_col = col\n",
    "                    break\n",
    "            \n",
    "            # 2. Find the Title Column (Optional, for display)\n",
    "            candidates_title = ['Job Title', 'Title', 'job_title', 'position']\n",
    "            for col in candidates_title:\n",
    "                if col in df.columns:\n",
    "                    title_col = col\n",
    "                    break\n",
    "            \n",
    "            # If we found a text column, clean it and add to our list\n",
    "            if text_col:\n",
    "                # Fill NaNs\n",
    "                df[text_col] = df[text_col].fillna(\"\")\n",
    "                if title_col:\n",
    "                    df[title_col] = df[title_col].fillna(\"Unknown Role\")\n",
    "                \n",
    "                # Standardize DataFrame\n",
    "                temp_df = pd.DataFrame()\n",
    "                temp_df['Job_Text'] = df[text_col].apply(clean_text)\n",
    "                \n",
    "                # Create a display title: \"Software Engineer (ID: 1)\"\n",
    "                if title_col:\n",
    "                    temp_df['Job_Title'] = df[title_col]\n",
    "                else:\n",
    "                    temp_df['Job_Title'] = \"Job Role \" + temp_df.index.astype(str)\n",
    "                \n",
    "                all_jobs.append(temp_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if all_jobs:\n",
    "        return pd.concat(all_jobs, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e39815f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_semantic_matching(resumes_df, jobs_df):\n",
    "    \n",
    "    # 1. Load Model\n",
    "    print(\"\\nLoading SBERT Model...\")\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    # 2. Embed Resumes\n",
    "    print(f\"Encoding {len(resumes_df)} Resumes...\")\n",
    "    resume_embeddings = model.encode(resumes_df['Resume_Text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # 3. Embed Job Descriptions\n",
    "    print(f\"Encoding {len(jobs_df)} Job Descriptions...\")\n",
    "    job_embeddings = model.encode(jobs_df['Job_Text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    # 4. Calculate Similarity Matrix\n",
    "    # Result is a Matrix of shape (Num_Jobs, Num_Resumes)\n",
    "    print(\"Calculating Cosine Similarity Matrix...\")\n",
    "    similarity_matrix = cosine_similarity(job_embeddings, resume_embeddings)\n",
    "    \n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3beb808c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Resumes...\n",
      "--- Scanning Resumes in archive(1)/data/data ---\n",
      "Total Resumes Loaded: 2483\n",
      "Loading Job Descriptions...\n",
      "--- Scanning Job Descriptions in jobdesc/home/sdf ---\n",
      "Loaded marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv with columns: ['Job Title', 'Job Description', 'Job Type', 'Categories', 'Location', 'City', 'State', 'Country', 'Zip Code', 'Address', 'Salary From', 'Salary To', 'Salary Period', 'Apply Url', 'Apply Email', 'Employees', 'Industry', 'Company Name', 'Employer Email', 'Employer Website', 'Employer Phone', 'Employer Logo', 'Companydescription', 'Employer Location', 'Employer City', 'Employer State', 'Employer Country', 'Employer Zip Code', 'Uniq Id', 'Crawl Timestamp']\n",
      "\n",
      "[INFO] Dataset is large (30002 jobs). Using first 50 for testing.\n",
      "\n",
      "Loading SBERT Model...\n",
      "Encoding 2483 Resumes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 78/78 [00:40<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding 50 Job Descriptions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:00<00:00,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating Cosine Similarity Matrix...\n",
      "\n",
      "================================================================================\n",
      " MATCHING RESULTS \n",
      "================================================================================\n",
      "\n",
      "JOB: Shift Manager\n",
      "----------------------------------------\n",
      "  Rank 1: 61.6% Match | SALES | 92200491.pdf\n",
      "  Rank 2: 61.5% Match | CHEF | 21334981.pdf\n",
      "  Rank 3: 59.3% Match | CHEF | 13212436.pdf\n",
      "\n",
      "JOB: Operations Support Manager\n",
      "----------------------------------------\n",
      "  Rank 1: 69.3% Match | CHEF | 13212436.pdf\n",
      "  Rank 2: 67.1% Match | CHEF | 21334981.pdf\n",
      "  Rank 3: 64.6% Match | CHEF | 11444536.pdf\n",
      "\n",
      "JOB: Senior Product Manager - Data\n",
      "----------------------------------------\n",
      "  Rank 1: 62.3% Match | DIGITAL-MEDIA | 62700506.pdf\n",
      "  Rank 2: 61.5% Match | DIGITAL-MEDIA | 27080812.pdf\n",
      "  Rank 3: 59.5% Match | DIGITAL-MEDIA | 28679359.pdf\n",
      "\n",
      "JOB: Part-Time Office Concierge\n",
      "----------------------------------------\n",
      "  Rank 1: 57.3% Match | BANKING | 16407619.pdf\n",
      "  Rank 2: 54.9% Match | SALES | 30529547.pdf\n",
      "  Rank 3: 54.9% Match | HR | 16852973.pdf\n",
      "\n",
      "JOB: Print & Marketing Associate\n",
      "----------------------------------------\n",
      "  Rank 1: 60.2% Match | SALES | 33236701.pdf\n",
      "  Rank 2: 58.6% Match | SALES | 13812481.pdf\n",
      "  Rank 3: 57.4% Match | SALES | 13637605.pdf\n",
      "\n",
      "Saving results for 50 jobs to 'final_matching_results.csv'...\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. LOAD DATA\n",
    "    print(\"Loading Resumes...\")\n",
    "    df_resumes = load_resumes(PATH_RESUMES)\n",
    "    \n",
    "    print(\"Loading Job Descriptions...\")\n",
    "    df_jobs = load_job_descriptions(PATH_JOBS)\n",
    "    \n",
    "    # --- SAFETY CHECK: LIMIT DATA SIZE ---\n",
    "    # 30,000 jobs is too many for a quick test. \n",
    "    # Let's slice it to the first 50 jobs to ensure the code works first.\n",
    "    if len(df_jobs) > 50:\n",
    "        print(f\"\\n[INFO] Dataset is large ({len(df_jobs)} jobs). Using first 50 for testing.\")\n",
    "        df_jobs = df_jobs.head(50)\n",
    "    # -------------------------------------\n",
    "    \n",
    "    if df_resumes.empty or df_jobs.empty:\n",
    "        print(\"Error: One of the datasets is empty. Please check paths.\")\n",
    "    else:\n",
    "        # 2. RUN MATCHING\n",
    "        # This will now compare (50 Jobs) x (All Resumes)\n",
    "        sim_matrix = run_semantic_matching(df_resumes, df_jobs)\n",
    "        \n",
    "        # 3. DISPLAY TOP MATCHES\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\" MATCHING RESULTS \")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Show top matches for the first 5 jobs\n",
    "        num_jobs_to_show = min(5, len(df_jobs))\n",
    "        \n",
    "        for job_idx in range(num_jobs_to_show):\n",
    "            # Safe access to title (handles cases where title might be missing)\n",
    "            job_title = df_jobs.iloc[job_idx]['Job_Title']\n",
    "            print(f\"\\nJOB: {job_title}\")\n",
    "            print(\"-\" * 40)\n",
    "            \n",
    "            # Get scores for this specific job\n",
    "            job_scores = sim_matrix[job_idx]\n",
    "            \n",
    "            # Sort indices by score (Descending)\n",
    "            # Get top 3 best resumes for this job\n",
    "            top_indices = np.argsort(job_scores)[::-1][:3] \n",
    "            \n",
    "            for rank, resume_idx in enumerate(top_indices):\n",
    "                score = job_scores[resume_idx]\n",
    "                resume_name = df_resumes.iloc[resume_idx]['Resume_ID']\n",
    "                category = df_resumes.iloc[resume_idx]['Category']\n",
    "                \n",
    "                print(f\"  Rank {rank+1}: {score*100:.1f}% Match | {category} | {resume_name}\")\n",
    "\n",
    "        # Optional: Save ALL results to a CSV\n",
    "        print(f\"\\nSaving results for {len(df_jobs)} jobs to 'final_matching_results.csv'...\")\n",
    "        \n",
    "        # Create a list to store all match rows\n",
    "        results_data = []\n",
    "        for i in range(len(df_jobs)):\n",
    "            job_title = df_jobs.iloc[i]['Job_Title']\n",
    "            job_scores = sim_matrix[i]\n",
    "            best_resume_idx = np.argmax(job_scores)\n",
    "            best_score = job_scores[best_resume_idx]\n",
    "            best_resume_name = df_resumes.iloc[best_resume_idx]['Resume_ID']\n",
    "            \n",
    "            results_data.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Best Resume\": best_resume_name,\n",
    "                \"Match Score\": best_score\n",
    "            })\n",
    "        \n",
    "        pd.DataFrame(results_data).to_csv(\"final_matching_results.csv\", index=False)\n",
    "        print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5a45d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DATA PROCESSING ENGINE\n",
      "========================================\n",
      "--- Scanning Resumes in archive(1)/data/data ---\n",
      "Processed 100 resumes...\n",
      "Processed 200 resumes...\n",
      "Processed 300 resumes...\n",
      "Processed 400 resumes...\n",
      "Processed 500 resumes...\n",
      "Processed 600 resumes...\n",
      "Processed 700 resumes...\n",
      "Processed 800 resumes...\n",
      "Processed 900 resumes...\n",
      "Processed 1000 resumes...\n",
      "Processed 1100 resumes...\n",
      "Processed 1200 resumes...\n",
      "Processed 1300 resumes...\n",
      "Processed 1400 resumes...\n",
      "Processed 1500 resumes...\n",
      "Processed 1600 resumes...\n",
      "Processed 1700 resumes...\n",
      "Processed 1800 resumes...\n",
      "Processed 1900 resumes...\n",
      "Processed 2000 resumes...\n",
      "Processed 2100 resumes...\n",
      "Processed 2200 resumes...\n",
      "Processed 2300 resumes...\n",
      "Processed 2400 resumes...\n",
      "Done! Loaded 2483 total resumes.\n",
      "--- Scanning Job Descriptions in jobdesc/home/sdf ---\n",
      "Loaded marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv with columns: ['Job Title', 'Job Description', 'Job Type', 'Categories', 'Location', 'City', 'State', 'Country', 'Zip Code', 'Address', 'Salary From', 'Salary To', 'Salary Period', 'Apply Url', 'Apply Email', 'Employees', 'Industry', 'Company Name', 'Employer Email', 'Employer Website', 'Employer Phone', 'Employer Logo', 'Companydescription', 'Employer Location', 'Employer City', 'Employer State', 'Employer Country', 'Employer Zip Code', 'Uniq Id', 'Crawl Timestamp']\n",
      "\n",
      "Loading AI Model (this happens only once)...\n",
      "Calculating vectors for 2483 resumes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 78/78 [00:49<00:00,  1.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating vectors for 100 jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data to 'processed_data.pkl'...\n",
      "\n",
      "SUCCESS! You never have to run this script again.\n",
      "Run 'streamlit run app.py' now to see the instant app.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz  # PyMuPDF\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "PATH_RESUMES = \"archive(1)/data/data\"\n",
    "PATH_JOBS = \"jobdesc/home/sdf\"\n",
    "OUTPUT_FILE = \"processed_data.pkl\"\n",
    "\n",
    "# Use the fast model\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "def clean_text(text):\n",
    "    if not text: return \"\"\n",
    "    text = str(text).replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r' ', text)\n",
    "    return re.sub(' +', ' ', text).strip()\n",
    "\n",
    "def load_resumes(base_path):\n",
    "    print(f\"--- Scanning Resumes in {base_path} ---\")\n",
    "    data = []\n",
    "    total_files = 0\n",
    "    \n",
    "    # Walk through all folders (Accountant, Engineering, etc.)\n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        category = os.path.basename(root)\n",
    "        if root == base_path: continue\n",
    "        \n",
    "        pdf_files = [f for f in files if f.lower().endswith(\".pdf\")]\n",
    "        \n",
    "        for file in pdf_files:\n",
    "            try:\n",
    "                # Read PDF\n",
    "                full_path = os.path.join(root, file)\n",
    "                with fitz.open(full_path) as doc:\n",
    "                    text = \"\".join([page.get_text() for page in doc])\n",
    "                \n",
    "                # Only save if it has text\n",
    "                if len(text) > 50:\n",
    "                    data.append({\n",
    "                        \"Filename\": file, \n",
    "                        \"Category\": category, \n",
    "                        \"Text\": clean_text(text),\n",
    "                        \"Path\": full_path\n",
    "                    })\n",
    "                    total_files += 1\n",
    "                    \n",
    "                    if total_files % 100 == 0:\n",
    "                        print(f\"Processed {total_files} resumes...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                \n",
    "    print(f\"Done! Loaded {len(data)} total resumes.\")\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# --- YOUR CUSTOM FUNCTION HERE ---\n",
    "def load_job_descriptions(base_path):\n",
    "    \"\"\"\n",
    "    Scans the folder for CSV files and loads Job Descriptions.\n",
    "    It attempts to find columns like 'Job Description', 'Description', or 'Job Title'.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(base_path):\n",
    "        print(f\"Warning: Job Desc folder '{base_path}' not found.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    print(f\"--- Scanning Job Descriptions in {base_path} ---\")\n",
    "    all_jobs = []\n",
    "\n",
    "    # Find all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(base_path) if f.lower().endswith('.csv')]\n",
    "    \n",
    "    for file in csv_files:\n",
    "        file_path = os.path.join(base_path, file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"Loaded {file} with columns: {list(df.columns)}\")\n",
    "            \n",
    "            # --- INTELLIGENT COLUMN DETECTION ---\n",
    "            text_col = None\n",
    "            title_col = None\n",
    "            \n",
    "            # 1. Find the Text Column\n",
    "            candidates_text = ['Job Description', 'Description', 'job_description', 'description', 'Job Text']\n",
    "            for col in candidates_text:\n",
    "                if col in df.columns:\n",
    "                    text_col = col\n",
    "                    break\n",
    "            \n",
    "            # 2. Find the Title Column (Optional, for display)\n",
    "            candidates_title = ['Job Title', 'Title', 'job_title', 'position']\n",
    "            for col in candidates_title:\n",
    "                if col in df.columns:\n",
    "                    title_col = col\n",
    "                    break\n",
    "            \n",
    "            # If we found a text column, clean it and add to our list\n",
    "            if text_col:\n",
    "                # Fill NaNs\n",
    "                df[text_col] = df[text_col].fillna(\"\")\n",
    "                if title_col:\n",
    "                    df[title_col] = df[title_col].fillna(\"Unknown Role\")\n",
    "                \n",
    "                # Standardize DataFrame\n",
    "                temp_df = pd.DataFrame()\n",
    "                temp_df['Job_Text'] = df[text_col].apply(clean_text)\n",
    "                \n",
    "                # Create a display title: \"Software Engineer (ID: 1)\"\n",
    "                if title_col:\n",
    "                    temp_df['Job_Title'] = df[title_col]\n",
    "                else:\n",
    "                    temp_df['Job_Title'] = \"Job Role \" + temp_df.index.astype(str)\n",
    "                \n",
    "                all_jobs.append(temp_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "    if all_jobs:\n",
    "        full_df = pd.concat(all_jobs, ignore_index=True)\n",
    "        return full_df.head(100) # <--- Keep this for a fast, smooth demo\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"STARTING DATA PROCESSING ENGINE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Load All Text Data\n",
    "    df_resumes = load_resumes(PATH_RESUMES)\n",
    "    \n",
    "    # CALLING YOUR FUNCTION HERE\n",
    "    df_jobs = load_job_descriptions(PATH_JOBS)\n",
    "    \n",
    "    if df_resumes.empty:\n",
    "        print(\"CRITICAL ERROR: No resumes found! Check your path.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Pre-Calculate Math (Embeddings)\n",
    "    print(\"\\nLoading AI Model (this happens only once)...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "    print(f\"Calculating vectors for {len(df_resumes)} resumes...\")\n",
    "    resume_vectors = model.encode(df_resumes['Text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    print(f\"Calculating vectors for {len(df_jobs)} jobs...\")\n",
    "    # Check if jobs were found before encoding\n",
    "    if not df_jobs.empty:\n",
    "        job_vectors = model.encode(df_jobs['Job_Text'].tolist(), show_progress_bar=True)\n",
    "    else:\n",
    "        print(\"Warning: No jobs found to encode. App will only work for custom pasted jobs.\")\n",
    "        job_vectors = []\n",
    "    \n",
    "    # 3. Save to Disk\n",
    "    print(f\"\\nSaving data to '{OUTPUT_FILE}'...\")\n",
    "    with open(OUTPUT_FILE, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"resumes\": df_resumes,\n",
    "            \"resume_vectors\": resume_vectors,\n",
    "            \"jobs\": df_jobs,\n",
    "            \"job_vectors\": job_vectors\n",
    "        }, f)\n",
    "        \n",
    "    print(\"\\nSUCCESS! You never have to run this script again.\")\n",
    "    print(f\"Run 'streamlit run app.py' now to see the instant app.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8e8ede6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DETAILED ACCURACY BREAKDOWN ---\n",
      "CATEGORY                  | ACCURACY   | COUNT\n",
      "--------------------------------------------------\n",
      "ACCOUNTANT                |  63.56%    | 118/118\n",
      "ADVOCATE                  |   0.00%    | 118/118\n",
      "AGRICULTURE               |   0.00%    | 63/63\n",
      "APPAREL                   |   0.00%    | 97/97\n",
      "ARTS                      |   5.83%    | 103/103\n",
      "AUTOMOBILE                |   0.00%    | 36/36\n",
      "AVIATION                  |  25.64%    | 117/117\n",
      "BANKING                   |  57.39%    | 115/115\n",
      "BPO                       |   0.00%    | 22/22\n",
      "BUSINESS-DEVELOPMENT      |  99.16%    | 119/119\n",
      "CHEF                      |   0.00%    | 118/118\n",
      "CONSTRUCTION              |   1.79%    | 112/112\n",
      "CONSULTANT                |  26.09%    | 115/115\n",
      "DESIGNER                  |   0.00%    | 107/107\n",
      "DIGITAL-MEDIA             |   0.00%    | 96/96\n",
      "ENGINEERING               |  76.27%    | 118/118\n",
      "FINANCE                   |   5.93%    | 118/118\n",
      "FITNESS                   |   0.00%    | 117/117\n",
      "HEALTHCARE                |   0.00%    | 115/115\n",
      "HR                        |  89.09%    | 110/110\n",
      "INFORMATION-TECHNOLOGY    |  81.67%    | 120/120\n",
      "PUBLIC-RELATIONS          |  67.57%    | 111/111\n",
      "SALES                     |  75.00%    | 116/116\n",
      "TEACHER                   |   0.00%    | 102/102\n",
      "--------------------------------------------------\n",
      "OVERALL ACCURACY: 31.49%\n"
     ]
    }
   ],
   "source": [
    "def calculate_accuracy_detailed(resume_vectors, job_vectors, df_resumes, df_jobs):\n",
    "    print(\"\\n--- DETAILED ACCURACY BREAKDOWN ---\")\n",
    "    \n",
    "    similarity_matrix = cosine_similarity(resume_vectors, job_vectors)\n",
    "    \n",
    "    # Use the same map from before\n",
    "    category_map = {\n",
    "        \"information-technology\": [\"it\", \"tech\", \"software\", \"developer\", \"data\", \"analyst\"],\n",
    "        \"business-development\": [\"business\", \"development\", \"manager\", \"sales\", \"growth\"],\n",
    "        \"accountant\": [\"account\", \"tax\", \"audit\", \"finance\", \"cpa\"],\n",
    "        \"advocate\": [\"legal\", \"law\", \"attorney\", \"counsel\"],\n",
    "        \"chef\": [\"chef\", \"cook\", \"culinary\", \"kitchen\"],\n",
    "        \"engineering\": [\"engineer\", \"mechanical\", \"electrical\"],\n",
    "        \"finance\": [\"finance\", \"banking\", \"investment\"],\n",
    "        \"aviation\": [\"pilot\", \"aviation\", \"flight\", \"aircraft\"],\n",
    "        \"fitness\": [\"fitness\", \"gym\", \"trainer\", \"coach\"],\n",
    "        \"sales\": [\"sales\", \"account executive\", \"rep\"],\n",
    "        \"healthcare\": [\"health\", \"medical\", \"nurse\", \"doctor\"],\n",
    "        \"consultant\": [\"consultant\", \"advisor\"],\n",
    "        \"banking\": [\"bank\", \"lending\", \"credit\"],\n",
    "        \"construction\": [\"construction\", \"site\", \"project manager\"],\n",
    "        \"public-relations\": [\"pr\", \"public relations\", \"media\"],\n",
    "        \"hr\": [\"human resources\", \"recruiter\", \"talent\"],\n",
    "        \"designer\": [\"design\", \"creative\", \"art\", \"graphic\", \"ux\", \"ui\"],\n",
    "        \"arts\": [\"art\", \"creative\", \"design\", \"gallery\"]\n",
    "    }\n",
    "\n",
    "    category_stats = {} # Store correct/total per category\n",
    "\n",
    "    for i in range(len(df_resumes)):\n",
    "        cat = str(df_resumes.iloc[i]['Category']).strip()\n",
    "        cat_key = cat.lower()\n",
    "        \n",
    "        if cat not in category_stats:\n",
    "            category_stats[cat] = {\"total\": 0, \"correct\": 0}\n",
    "            \n",
    "        category_stats[cat][\"total\"] += 1\n",
    "        \n",
    "        # Check Top 3 Matches\n",
    "        top_3_indices = np.argsort(similarity_matrix[i])[-3:][::-1]\n",
    "        \n",
    "        found_match = False\n",
    "        for idx in top_3_indices:\n",
    "            pred_title = str(df_jobs.iloc[idx]['Job_Title']).lower()\n",
    "            \n",
    "            if cat_key in pred_title:\n",
    "                found_match = True\n",
    "            elif cat_key in category_map:\n",
    "                for keyword in category_map[cat_key]:\n",
    "                    if keyword in pred_title:\n",
    "                        found_match = True\n",
    "            if found_match: break\n",
    "        \n",
    "        if found_match:\n",
    "            category_stats[cat][\"correct\"] += 1\n",
    "\n",
    "    # Print Report\n",
    "    print(f\"{'CATEGORY':<25} | {'ACCURACY':<10} | {'COUNT'}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    total_correct = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    for cat, stats in category_stats.items():\n",
    "        acc = (stats[\"correct\"] / stats[\"total\"]) * 100\n",
    "        print(f\"{cat:<25} | {acc:6.2f}%    | {stats['total']}/{stats['total']}\")\n",
    "        \n",
    "        total_correct += stats[\"correct\"]\n",
    "        total_count += stats[\"total\"]\n",
    "        \n",
    "    print(\"-\" * 50)\n",
    "    print(f\"OVERALL ACCURACY: {(total_correct/total_count)*100:.2f}%\")\n",
    "\n",
    "# Run it\n",
    "calculate_accuracy_detailed(resume_vectors, job_vectors, df_resumes, df_jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81ba780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING DATA PROCESSING ENGINE\n",
      "========================================\n",
      "--- Scanning Resumes in archive(1)/data/data ---\n",
      "Processed 100 resumes...\n",
      "Processed 200 resumes...\n",
      "Processed 300 resumes...\n",
      "Processed 400 resumes...\n",
      "Processed 500 resumes...\n",
      "Processed 600 resumes...\n",
      "Processed 700 resumes...\n",
      "Processed 800 resumes...\n",
      "Processed 900 resumes...\n",
      "Processed 1000 resumes...\n",
      "Processed 1100 resumes...\n",
      "Processed 1200 resumes...\n",
      "Processed 1300 resumes...\n",
      "Processed 1400 resumes...\n",
      "Processed 1500 resumes...\n",
      "Processed 1600 resumes...\n",
      "Processed 1700 resumes...\n",
      "Processed 1800 resumes...\n",
      "Processed 1900 resumes...\n",
      "Processed 2000 resumes...\n",
      "Processed 2100 resumes...\n",
      "Processed 2200 resumes...\n",
      "Processed 2300 resumes...\n",
      "Processed 2400 resumes...\n",
      "Done! Loaded 2483 total resumes.\n",
      "--- Scanning Job Descriptions in jobdesc/home/sdf ---\n",
      "Loaded marketing_sample_for_trulia_com-real_estate__20190901_20191031__30k_data.csv with columns: ['Job Title', 'Job Description', 'Job Type', 'Categories', 'Location', 'City', 'State', 'Country', 'Zip Code', 'Address', 'Salary From', 'Salary To', 'Salary Period', 'Apply Url', 'Apply Email', 'Employees', 'Industry', 'Company Name', 'Employer Email', 'Employer Website', 'Employer Phone', 'Employer Logo', 'Companydescription', 'Employer Location', 'Employer City', 'Employer State', 'Employer Country', 'Employer Zip Code', 'Uniq Id', 'Crawl Timestamp']\n",
      "\n",
      "Loading AI Model (this happens only once)...\n",
      "Calculating vectors for 2483 resumes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 78/78 [00:40<00:00,  1.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating vectors for 100 jobs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 4/4 [00:01<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CALCULATING SMART ACCURACY ---\n",
      "Testing 2483 resumes against 100 jobs...\n",
      "Smart Accuracy: 35.72%\n",
      "\n",
      "Saving data to 'processed_data.pkl'...\n",
      "\n",
      "SUCCESS! Data processing complete.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"STARTING DATA PROCESSING ENGINE\")\n",
    "    print(\"=\"*40)\n",
    "    \n",
    "    # 1. Load All Text Data\n",
    "    df_resumes = load_resumes(PATH_RESUMES)\n",
    "    df_jobs = load_job_descriptions(PATH_JOBS)\n",
    "    \n",
    "    if df_resumes.empty:\n",
    "        print(\"CRITICAL ERROR: No resumes found! Check your path.\")\n",
    "        exit()\n",
    "\n",
    "    # 2. Pre-Calculate Math (Embeddings)\n",
    "    print(\"\\nLoading AI Model (this happens only once)...\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    \n",
    "    print(f\"Calculating vectors for {len(df_resumes)} resumes...\")\n",
    "    resume_vectors = model.encode(df_resumes['Text'].tolist(), show_progress_bar=True)\n",
    "    \n",
    "    print(f\"Calculating vectors for {len(df_jobs)} jobs...\")\n",
    "    if not df_jobs.empty:\n",
    "        job_vectors = model.encode(df_jobs['Job_Text'].tolist(), show_progress_bar=True)\n",
    "        \n",
    "        # --- HERE IS HOW YOU CALL IT ---\n",
    "        calculate_accuracy(resume_vectors, job_vectors, df_resumes, df_jobs)\n",
    "        # -------------------------------\n",
    "        \n",
    "    else:\n",
    "        print(\"Warning: No jobs found. Skipping accuracy check.\")\n",
    "        job_vectors = []\n",
    "    \n",
    "    # 3. Save to Disk\n",
    "    print(f\"\\nSaving data to '{OUTPUT_FILE}'...\")\n",
    "    with open(OUTPUT_FILE, 'wb') as f:\n",
    "        pickle.dump({\n",
    "            \"resumes\": df_resumes,\n",
    "            \"resume_vectors\": resume_vectors,\n",
    "            \"jobs\": df_jobs,\n",
    "            \"job_vectors\": job_vectors\n",
    "        }, f)\n",
    "        \n",
    "    print(\"\\nSUCCESS! Data processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c93f8a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- DATA DISTRIBUTION CHECK ---\n",
      "Total Resumes: 2483\n",
      "Total Jobs: 100\n",
      "\n",
      "--- RESUME COUNTS (By Folder) ---\n",
      "Category\n",
      "INFORMATION-TECHNOLOGY    120\n",
      "BUSINESS-DEVELOPMENT      119\n",
      "ACCOUNTANT                118\n",
      "ADVOCATE                  118\n",
      "CHEF                      118\n",
      "ENGINEERING               118\n",
      "FINANCE                   118\n",
      "AVIATION                  117\n",
      "FITNESS                   117\n",
      "SALES                     116\n",
      "HEALTHCARE                115\n",
      "CONSULTANT                115\n",
      "BANKING                   115\n",
      "CONSTRUCTION              112\n",
      "PUBLIC-RELATIONS          111\n",
      "HR                        110\n",
      "DESIGNER                  107\n",
      "ARTS                      103\n",
      "TEACHER                   102\n",
      "APPAREL                    97\n",
      "DIGITAL-MEDIA              96\n",
      "AGRICULTURE                63\n",
      "AUTOMOBILE                 36\n",
      "BPO                        22\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- JOB COUNTS (By Title) ---\n",
      "Job_Title\n",
      "Sales Associate/Beauty Advisor                3\n",
      "Lids Assistant Manager Full-Time              2\n",
      "Shift Manager                                 1\n",
      "Part-Time Office Concierge                    1\n",
      "Operations Support Manager                    1\n",
      "Cyber IT Risk & Strategy Senior Consultant    1\n",
      "Sales Associate, Retail Part Time             1\n",
      "Home Lending Branch Manager-Spokane           1\n",
      "Senior Product Manager - Data                 1\n",
      "Property Manager in Training (MIT)            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "# Load data\n",
    "with open(\"processed_data.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "df_resumes = data[\"resumes\"]\n",
    "df_jobs = data[\"jobs\"]\n",
    "\n",
    "print(f\"\\n--- DATA DISTRIBUTION CHECK ---\")\n",
    "print(f\"Total Resumes: {len(df_resumes)}\")\n",
    "print(f\"Total Jobs: {len(df_jobs)}\")\n",
    "\n",
    "# 1. Count Resumes per Category\n",
    "print(\"\\n--- RESUME COUNTS (By Folder) ---\")\n",
    "print(df_resumes['Category'].value_counts())\n",
    "\n",
    "# 2. Count Jobs per Title (First 10)\n",
    "print(\"\\n--- JOB COUNTS (By Title) ---\")\n",
    "print(df_jobs['Job_Title'].value_counts().head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
